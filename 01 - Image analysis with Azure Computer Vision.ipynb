{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Part I: Image analysis with Azure Computer Vision (REST API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install requests pillow matplotlib azure-cognitiveservices-vision-computervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626081206526
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "# If you are using a Jupyter Notebook, uncomment the following line.\n",
        "# %matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Setup Key and Endpoint\n",
        "\n",
        "To use your cognitive services resource, client applications need its endpoint and authentication key:\n",
        "\n",
        "1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
        "2. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
        "3. Run the code below by selecting the cell and then clicking the **Run cell** (&#9655) button to the left of the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626061508290
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Setup Key and end point\n",
        "subscription_key = 'YOUR_COG_KEY'\n",
        "endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "# api url\n",
        "analyze_url = endpoint + \"vision/v3.1/analyze\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3. Analyze image features\n",
        "1. You can use the Computer Vision service to generate a descriptive caption for a couple of images\n",
        "- Run the following code to analyze an image of a cat.\n",
        "- Check the result\n",
        "2. In the code below, change 'visualFeatures': 'Categories,Description,Color' to 'visualFeatures': 'Categories,Description,Color,Objects'\n",
        "- Run the following code to analyze an image of a cat.\n",
        "- Check the result\n",
        "\n",
        "For more detail, you can check the link below:\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/vision-api-how-to-topics/howtocallvisionapi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626062346186
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set image_url to the URL of an image that you want to analyze.\n",
        "image_url = \"https://c.files.bbci.co.uk/12A9B/production/_111434467_gettyimages-1143489763.jpg\"\n",
        "\n",
        "\n",
        "#send image to azure computer vision api\n",
        "headers = {\n",
        "    # Request headers\n",
        "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
        "}\n",
        "params = {\n",
        "    # specifiy what you want to find in the image\n",
        "    # Try to change 'Categories,Description,Color' to 'Categories,Description,Color,Objects'\n",
        "    'visualFeatures': 'Categories,Description,Color,Objects'\n",
        "}\n",
        "data = {'url': image_url}\n",
        "response = requests.post(analyze_url, headers=headers,\n",
        "                         params=params, json=data)\n",
        "\n",
        "\n",
        "\n",
        "# The 'analysis' object contains various fields that describe the image. The most\n",
        "# relevant caption for the image is obtained from the 'description' property.\n",
        "analysis = response.json()\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "# Make use of the result\n",
        "image_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()\n",
        "\n",
        "# Display the image and overlay it with the caption.\n",
        "image = Image.open(BytesIO(requests.get(image_url).content))\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "_ = plt.title(image_caption, size=\"x-large\", y=-0.1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 4. Bounding box\n",
        "After you changed the visualFeatures to detect objects in the image. You can see the result contain 3 information of the detected objects:\n",
        "\n",
        "1. The coordinate and width of bounding box\n",
        "2. Predicted label\n",
        "3. Confidence value\n",
        "\n",
        "You can use these information to draw bounding box for the objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626062166147
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def draw_bounding_box_on_image(image,\n",
        "                               obj_arr,\n",
        "                               color='red',\n",
        "                               thickness=4,\n",
        "                               use_normalized_coordinates=True):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    font_size = round(image.width*image.height*0.00006)\n",
        "    font = ImageFont.truetype('font/Roboto-Regular.ttf', font_size)\n",
        "\n",
        "\n",
        "    for obj in obj_arr:\n",
        "        box = obj[\"rectangle\"]\n",
        "        if 'x' in box:\n",
        "            xmin = box['x']\n",
        "        else:\n",
        "            xmin = 0\n",
        "\n",
        "        if 'w' in box:\n",
        "            xmax = xmin+box[\"w\"]\n",
        "        else:\n",
        "            xmax = box[\"w\"]\n",
        "\n",
        "        if 'y' in box:\n",
        "            ymin = box['y']\n",
        "        else:\n",
        "            ymin = 0\n",
        "\n",
        "        if 'h' in box:\n",
        "            ymax = ymin+box['h']\n",
        "        else:\n",
        "            ymax = 0\n",
        "\n",
        "        im_width, im_height = image.size\n",
        "        (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "\n",
        "        draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)\n",
        "        draw.text((xmin+10,ymin+10), \"Predict: {}\\nConfidence: {}\".format(obj[\"object\"], obj[\"confidence\"]), fill=(255,0,0), font = font)\n",
        "\n",
        "draw_bounding_box_on_image(image, analysis['objects'])\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Part II: Image analysis with Azure Computer Vision (Python library)\n",
        "\n",
        "If you want to use Azure serivce in your local computer. Please check the quickstart guide below.\n",
        "\n",
        "[Quickstart Guide](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/quickstarts-sdk/image-analysis-client-library?tabs=visual-studio&pivots=programming-language-python)\n",
        "\n",
        "\n",
        "## Notice: Change python version to 3.8\n",
        "### Get the Key and Endpoint for your Cognitive Services resource\n",
        "\n",
        "To use your cognitive services resource, client applications need its endpoint and authentication key:\n",
        "\n",
        "1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
        "2. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
        "3. Run the code below by selecting the cell and then clicking the **Run cell** (&#9655) button to the left of the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626334044645
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now that you've set up the key and endpoint, you can use the computer vision service to analyze an image.\n",
        "\n",
        "Run the following cell to get a description for an image in the */data/vision/store_cam1.jpg* file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626333707293
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import vision\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the path to an image file\n",
        "image_path = os.path.join('data', 'vision', 'store_cam1.jpg')\n",
        "\n",
        "# Get a client for the computer vision service\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Get a description from the computer vision service\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\n",
        "vision.show_image_caption(image_path, description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for c in description.captions:\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "That seems reasonably accurate.\n",
        "\n",
        "Let's try another image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626330976554
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the path to an image file\n",
        "image_path = os.path.join('data', 'vision', 'store_cam2.jpg')\n",
        "\n",
        "# Get a description from the computer vision service\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\n",
        "vision.show_image_caption(image_path, description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Again, the suggested caption seems to be pretty accurate.\n",
        "\n",
        "## Analyze image features\n",
        "\n",
        "So far, you've used the Computer Vision service to generate a descriptive caption for a couple of images; but there's much more you can do. The Computer Vision service provides analysis capabilities that can extract detailed information like:\n",
        "\n",
        "- The locations of common types of object detected in the image.\n",
        "- Location and approximate age of human faces in the image.\n",
        "- Whether the image contains any 'adult', 'racy', or 'gory' content.\n",
        "- Relevant tags that could be associated with the image in a database to make it easy to find.\n",
        "\n",
        "Run the following code to analyze an image of a shopper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1626330980761
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the path to an image file\n",
        "image_path = os.path.join('data', 'vision', 'strange.png')\n",
        "\n",
        "# Specify the features we want to analyze\n",
        "features = ['Description', 'Tags', 'Adult', 'Objects', 'Faces']\n",
        "\n",
        "# Get an analysis from the computer vision service\n",
        "image_stream = open(image_path, \"rb\")\n",
        "analysis = computervision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
        "\n",
        "# Show the results of analysis (code in helper_scripts/vision.py)\n",
        "vision.show_image_analysis(image_path, analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Learn More\n",
        "\n",
        "In addition to the capabilities you've explored in this notebook, the Computer Vision cognitive service includes the ability to:\n",
        "\n",
        "- Identify celebrities in images.\n",
        "- Detect brand logos in an image.\n",
        "- Perform optical character recognition (OCR) to read text in an image.\n",
        "\n",
        "To learn more about the Computer Vision cognitive service, see the [Computer Vision documentation](https://docs.microsoft.com/azure/cognitive-services/computer-vision/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "827db2d23f990fa2911f6b458046ca19ccdac263b09d9b97274b04da1fb74a12"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
