{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image analysis with Azure Computer Vision"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import required Libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# If you are using a Jupyter Notebook, uncomment the following line.\n",
        "# %matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from io import BytesIO"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626081206526
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setup Key and Endpoint\n",
        "\n",
        "To use your cognitive services resource, client applications need its endpoint and authentication key:\n",
        "\n",
        "1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
        "2. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
        "3. Run the code below by selecting the cell and then clicking the **Run cell** (&#9655) button to the left of the cell."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Key and end point\n",
        "subscription_key = 'YOUR_COG_KEY'\n",
        "endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "# api url\n",
        "analyze_url = endpoint + \"vision/v3.1/analyze\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626061508290
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Analyze image features\n",
        "1. You can use the Computer Vision service to generate a descriptive caption for a couple of images\n",
        "- Run the following code to analyze an image of a cat.\n",
        "- Check the result\n",
        "2. In the code below, change 'visualFeatures': 'Categories,Description,Color' to 'visualFeatures': 'Categories,Description,Color,Objects'\n",
        "- Run the following code to analyze an image of a cat.\n",
        "- Check the result\n",
        "\n",
        "For more detail, you can check the link below:\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/vision-api-how-to-topics/howtocallvisionapi\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image_url to the URL of an image that you want to analyze.\n",
        "image_url = \"https://c.files.bbci.co.uk/12A9B/production/_111434467_gettyimages-1143489763.jpg\"\n",
        "\n",
        "\n",
        "#send image to azure computer vision api\n",
        "headers = {\n",
        "    # Request headers\n",
        "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
        "}\n",
        "params = {\n",
        "    # specifiy what you want to find in the image\n",
        "    # Try to change 'Categories,Description,Color' to 'Categories,Description,Color,Objects'\n",
        "    'visualFeatures': 'Categories,Description,Color'\n",
        "}\n",
        "data = {'url': image_url}\n",
        "response = requests.post(analyze_url, headers=headers,\n",
        "                         params=params, json=data)\n",
        "\n",
        "\n",
        "\n",
        "# The 'analysis' object contains various fields that describe the image. The most\n",
        "# relevant caption for the image is obtained from the 'description' property.\n",
        "analysis = response.json()\n",
        "print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "# Make use of the result\n",
        "image_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()\n",
        "\n",
        "# Display the image and overlay it with the caption.\n",
        "image = Image.open(BytesIO(requests.get(image_url).content))\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "_ = plt.title(image_caption, size=\"x-large\", y=-0.1)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626062346186
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bounding box\n",
        "After you changed the visualFeatures to detect objects in the image. You can see the result contain 3 information of the detected objects:\n",
        "\n",
        "1. The coordinate and width of bounding box\n",
        "2. Predicted label\n",
        "3. Confidence value\n",
        "\n",
        "You can use these information to draw bounding box for the objects."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bounding_box_on_image(image,\n",
        "                               obj_arr,\n",
        "                               color='red',\n",
        "                               thickness=4,\n",
        "                               use_normalized_coordinates=True):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    font_size = round(image.width*image.height*0.00006)\n",
        "    font = ImageFont.truetype('font/Roboto-Regular.ttf', font_size)\n",
        "\n",
        "\n",
        "    for obj in obj_arr:\n",
        "        box = obj[\"rectangle\"]\n",
        "        if 'x' in box:\n",
        "            xmin = box['x']\n",
        "        else:\n",
        "            xmin = 0\n",
        "\n",
        "        if 'w' in box:\n",
        "            xmax = xmin+box[\"w\"]\n",
        "        else:\n",
        "            xmax = box[\"w\"]\n",
        "\n",
        "        if 'y' in box:\n",
        "            ymin = box['y']\n",
        "        else:\n",
        "            ymin = 0\n",
        "\n",
        "        if 'h' in box:\n",
        "            ymax = ymin+box['h']\n",
        "        else:\n",
        "            ymax = 0\n",
        "\n",
        "        im_width, im_height = image.size\n",
        "        (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "\n",
        "        draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)\n",
        "        draw.text((xmin+10,ymin+10), \"Predict: {}\\nConfidence: {}\".format(obj[\"object\"], obj[\"confidence\"]), fill=(255,0,0), font = font)\n",
        "\n",
        "draw_bounding_box_on_image(image, analysis['objects'])\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626062166147
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}